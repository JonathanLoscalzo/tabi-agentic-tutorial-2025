# ğŸ¯ Agentic Tabi QA

Sistema de preguntas y respuestas con RAG (Retrieval Augmented Generation) usando **Ollama**, **ChromaDB** y **LangChain**.

## ğŸ“‹ DescripciÃ³n

Este proyecto procesa documentos PDF y crea una base de datos vectorial que permite hacer preguntas sobre el contenido de los documentos usando modelos de lenguaje local con Ollama. Los agentes usan BedRock.

### CaracterÃ­sticas

- âœ… **Procesamiento de PDFs**: Carga y divide documentos en chunks manejables
- âœ… **Base de datos vectorial**: Usa ChromaDB para almacenamiento eficiente
- âœ… **Embeddings locales**: Genera embeddings con Ollama (nomic-embed-text)
- âœ… **RAG (Retrieval Augmented Generation)**: Responde preguntas con contexto relevante
- âœ… **100% Local**: Todo funciona en tu mÃ¡quina, sin APIs externas
- âœ… **Modo interactivo**: Interfaz de lÃ­nea de comandos para conversaciÃ³n

## ğŸ—ï¸ Estructura del Proyecto

```
agentic-tabi-qa/
â”œâ”€â”€ data/                           # PDFs a procesar
â”‚   ....
â”œâ”€â”€ src/                            # CÃ³digo fuente
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_loader.py          # Carga y procesa PDFs
â”‚   â”œâ”€â”€ vector_db.py                # GestiÃ³n de ChromaDB
â”‚   â””â”€â”€ qa_engine.py                # Motor de Q&A con RAG
â”œâ”€â”€ chroma_db/                      # Base de datos vectorial (generado)
â”œâ”€â”€ main.py                         # Script principal
â”œâ”€â”€ pyproject.toml                  # ConfiguraciÃ³n del proyecto
â”œâ”€â”€ requirements.txt                # Dependencias
â””â”€â”€ README.md                       # Este archivo
```

## ğŸš€ InstalaciÃ³n

### 1. Requisitos Previos

#### Instalar Ollama

**macOS:**

```bash
brew install ollama
```

**Linux:**

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
Descarga desde [ollama.com](https://ollama.com)

#### Iniciar Ollama

```bash
ollama serve
```

#### Descargar modelos necesarios

```bash
# Modelo para embeddings
ollama pull embeddinggema

# Modelo para generaciÃ³n de respuestas (elige uno)
ollama pull llama3.2      # Recomendado (ligero y rÃ¡pido)
# ollama pull llama3.1     # Alternativa mÃ¡s potente
# ollama pull mistral      # Otra alternativa
```

### 2. Instalar Dependencias de Python

- Instalar uv

```bash
uv pip install -r pyproject.toml
uv pip install -e .
```

## ğŸ“– Uso

### CLI

```
tabi-qa load
tabi-qa ask
```

### ğŸ¨ Interfaz Streamlit

Ejecutar

```
tabi-qa-st
```

### Agente

Ejecutar

```
tabi-qa-agent
```

## ğŸ“Š Â¿Por quÃ© ChromaDB?

**ChromaDB** fue seleccionada por las siguientes razones:

| CaracterÃ­stica         | ChromaDB                         | Alternativas                       |
| ---------------------- | -------------------------------- | ---------------------------------- |
| **Facilidad de uso**   | â­â­â­â­â­ Simple, sin servidor  | Pinecone/Weaviate requieren config |
| **Local-first**        | âœ… 100% local                    | Pinecone es cloud-only             |
| **IntegraciÃ³n Ollama** | âœ… Excelente                     | Variable                           |
| **Persistencia**       | âœ… AutomÃ¡tica en disco           | Algunos requieren setup            |
| **Performance**        | â­â­â­â­ RÃ¡pido para ~1000s docs | FAISS mÃ¡s rÃ¡pido pero mÃ¡s complejo |
| **Python API**         | â­â­â­â­â­ Muy Pythonic          | Variable                           |

### Otras opciones consideradas:

- **FAISS**: MÃ¡s rÃ¡pido pero requiere mÃ¡s configuraciÃ³n
- **Pinecone**: Excelente pero cloud-only (no local)
- **Weaviate**: Potente pero requiere Docker/servidor
- **Qdrant**: Bueno pero mÃ¡s complejo de configurar

## ğŸ§ª Ejemplos de Preguntas

Basado en los documentos incluidos, puedes preguntar:

### Modelado Dimensional

- "Â¿QuÃ© es el modelado dimensional?"
- "Â¿CuÃ¡les son las tablas de hechos y dimensiones?"
- "Â¿QuÃ© es un data warehouse?"
- "Explica el concepto de grano en modelado dimensional"

### AnÃ¡lisis de Datos con Software Libre

- "Â¿QuÃ© ventajas tiene usar software libre para anÃ¡lisis de datos?"
- "Â¿QuÃ© herramientas de software libre menciona el documento?"
- "Â¿CÃ³mo se compara Python con R para anÃ¡lisis de datos?"

## LangFuse

**LangFuse** es una plataforma open-source de observabilidad y anÃ¡lisis para aplicaciones LLM (Large Language Models). En este proyecto se utiliza para:

- ğŸ“Š **Tracing**: Rastrear y visualizar cada paso de la ejecuciÃ³n de los agentes (llamadas al LLM, uso de herramientas, etc.)
- ğŸ” **Debugging**: Identificar problemas en el flujo de los agentes y optimizar prompts
- ğŸ“ˆ **MÃ©tricas**: Monitorear el rendimiento, costos, latencia y calidad de las respuestas
- ğŸ§ª **EvaluaciÃ³n**: Comparar diferentes versiones de prompts y configuraciones

El sistema estÃ¡ integrado con LangFuse mediante el `CallbackHandler` de LangChain, lo que permite observabilidad completa sin modificar la lÃ³gica de los agentes.

## ğŸ› SoluciÃ³n de Problemas

### Respuestas de baja calidad

- Ajusta `chunk_size` y `chunk_overlap` en `DocumentLoader`
- Incrementa `n_context_docs` en `QAEngine`
- Usa un modelo mÃ¡s potente (llama3.1 en lugar de llama3.2)
- Ajusta la `temperature` (valores mÃ¡s bajos = mÃ¡s conservador)

## ğŸ“ Notas TÃ©cnicas

### Proceso de RAG

1. **Carga de documentos**: Los PDFs se extraen y dividen en chunks
2. **GeneraciÃ³n de embeddings**: Cada chunk se convierte en un vector usando `nomic-embed-text`
3. **Almacenamiento**: Los vectores se guardan en ChromaDB con sus metadatos
4. **Consulta**: Cuando haces una pregunta:
   - Se genera un embedding de la pregunta
   - Se buscan los chunks mÃ¡s similares (cosine similarity)
   - Se construye un prompt con el contexto relevante
   - El LLM genera una respuesta basada en ese contexto

## ğŸ“„ Licencia

libre de usar y modificar este cÃ³digo.

## Referencias
- https://docs.langchain.com/oss/python/langgraph/graph-api
- https://docs.langchain.com/oss/python/langgraph/workflows-agents

